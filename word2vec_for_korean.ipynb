{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMx4KC8eNj/ofmTY4w5jGa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seojeongyun/Word2Vec/blob/main/word2vec_for_korean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGHAjc6mko8X",
        "outputId": "c6cbd880-0592-4196-941b-e997395559ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"/content/drive/My Drive/ratings_train.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ9fydY0k3hU",
        "outputId": "8743eb36-36c5-4492-bc29-4e1aaa0d42a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 20] Not a directory: '/content/drive/My Drive/ratings_train.txt'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/seojeongyun/naver_movie_review_sentiment_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOyCFIPOk3oP",
        "outputId": "c77fc28d-0aad-454f-8d0f-10fd31a805a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'naver_movie_review_sentiment_analysis'...\n",
            "remote: Enumerating objects: 22, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 22 (delta 3), reused 12 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (22/22), 8.09 MiB | 1.52 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Data download from url\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_train.txt\")\n",
        "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\",\n",
        "                               filename=\"/content/drive/My Drive/ratings_test.txt\")"
      ],
      "metadata": {
        "id": "8iMWdTA2k3qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEtOVHR8mZk0",
        "outputId": "a3900160-0ba4-4674-d7b3-01b167111129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.3)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import urllib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "class preprocess:\n",
        "    def __init__(self):\n",
        "        # By using pandas, data save\n",
        "        self.train_data = pd.read_table('/content/drive/My Drive/ratings_train.txt')\n",
        "        self.test_data = pd.read_table('/content/drive/My Drive/ratings_test.txt')\n",
        "        self.stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
        "        self.Okt = Okt()\n",
        "\n",
        "    def dataset_len(self, type: str):  # Check the number of datasets\n",
        "        if type == 'train':\n",
        "            print('The number of train reviews : ', len(self.train_data))\n",
        "        else:\n",
        "            print('The number of test reviews : ', len(self.test_data))\n",
        "\n",
        "    def process(self, type: str):\n",
        "        if type == 'train':\n",
        "            # Remove duplications column of document\n",
        "            self.train_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            train_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # Remove special characters with regular expression\n",
        "            train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
        "            train_data[:5]\n",
        "\n",
        "            # Change the white space value to Null value and then remove\n",
        "            train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            train_data['document'].replace('', np.nan, inplace=True)\n",
        "            print(train_data.isnull().sum())\n",
        "            train_data = train_data.dropna(how='any')\n",
        "\n",
        "            y_train = np.array(train_data['label']) # get labels\n",
        "            return train_data, y_train\n",
        "\n",
        "        else:\n",
        "            # Apply same preprocess to test dataset\n",
        "            self.test_data.drop_duplicates(subset=['document'], inplace=True)\n",
        "\n",
        "            # Remove the samples with null value\n",
        "            test_data = self.train_data.dropna(how='any')\n",
        "\n",
        "            # remove duplicate value for column of ducument\n",
        "            test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
        "            test_data[:5]\n",
        "\n",
        "            test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
        "            test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
        "            test_data = test_data.dropna(how='any')  # remove the null value\n",
        "            y_test = np.array(test_data['label'])\n",
        "            return test_data, y_test\n",
        "\n",
        "    def check_duplication(self):  # Check duplications column of document and label\n",
        "        self.train_data['document'].nunique(), self.train_data['label'].nunique()\n",
        "        self.test_data['document'].nunique(), self.test_data['label'].nunique()\n",
        "\n",
        "    def remove_stopword(self, type: str, data):\n",
        "        if type == 'train':\n",
        "            X_train = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = (self.Okt.morphs(sentence, stem=True))  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_train.append(stopwords_removed_sentence)\n",
        "            return X_train\n",
        "\n",
        "        else:\n",
        "            X_test = []\n",
        "            for sentence in tqdm(data['document']):\n",
        "                tokenized_sentence = self.Okt.morphs(sentence, stem=True)  # 토큰화\n",
        "                stopwords_removed_sentence = [word for word in tokenized_sentence if not word in self.stopwords]  # 불용어 제거\n",
        "                X_test.append(stopwords_removed_sentence)\n",
        "            return X_test\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     data = preprocess()\n",
        "\n",
        "#     data.dataset_len('train')\n",
        "#     data.dataset_len('test')\n",
        "\n",
        "#     data.check_duplication()\n",
        "\n",
        "#     data.process('train')\n",
        "#     data.process('test')\n",
        "\n",
        "#     # Check the ratio of train and labels\n",
        "#     data.train_data['label'].value_counts()\n",
        "\n",
        "#     # Check the null value from train set\n",
        "#     print(data.train_data.isnull().values.any())\n",
        "#     print(data.train_data.isnull().sum())\n",
        "#     data.train_data.loc[data.train_data.document.isnull()]\n",
        "\n",
        "#     print('The number of test dataset after preprocess :',len(data.test_data))"
      ],
      "metadata": {
        "id": "ytSDWI3oliad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ### 클래스 선언\n",
        "    token = Tokenize()\n",
        "    data = preprocess()"
      ],
      "metadata": {
        "id": "xMNahejrGRSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    ### 데이터 전처리\n",
        "    X_train, Y_train = data.process('train')\n",
        "    X_test, Y_test = data.process('test')\n",
        "\n",
        "    ## 불용어 제거\n",
        "    X_train_removed = data.remove_stopword('train', X_train)\n",
        "    X_test_removed = data.remove_stopword('test', X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_phtLo0DzrRa",
        "outputId": "fb9c5409-aaa1-404f-934d-6e98dbf19940"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4fa79e7dfdbd>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-7-4fa79e7dfdbd>:35: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
            "<ipython-input-7-4fa79e7dfdbd>:39: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['document'].replace('', np.nan, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id            0\n",
            "document    789\n",
            "label         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4fa79e7dfdbd>:55: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-7-4fa79e7dfdbd>:55: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")  # Apply regular expression\n",
            "<ipython-input-7-4fa79e7dfdbd>:58: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'] = self.test_data['document'].str.replace('^ +', \"\")  # change the white space to empty value\n",
            "<ipython-input-7-4fa79e7dfdbd>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['document'].replace('', np.nan, inplace=True)  # change epmty space to null value\n",
            "100%|██████████| 145393/145393 [05:33<00:00, 436.61it/s]\n",
            "100%|██████████| 48297/48297 [01:32<00:00, 521.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAY1EhfOzgEZ",
        "outputId": "cd79ee70-e3a6-4138-9581-622dfaeb9ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합(vocabulary)의 크기 : 43752\n",
            "등장 빈도가 2번 이하인 희귀 단어의 수: 24337\n",
            "단어 집합에서 희귀 단어의 비율: 55.62488571950996\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.0\n",
            "단어 집합의 크기 : 19416\n",
            "145393\n",
            "###############################\n",
            "vocabsize로 토크나이저설정\n",
            "핏온텍스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "핏온텍스트\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/lib/function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr = asarray(arr)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = X_train_removed, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
      ],
      "metadata": {
        "id": "v2SGoCTRxWre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv.most_similar(\"최민식\"))"
      ],
      "metadata": {
        "id": "mNbvGfsRtHkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}